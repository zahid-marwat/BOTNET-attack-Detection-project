{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84175aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551b6a3706474db0bbd8def5db56b673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, bar_style='info', description='Stage', max=6), HTML(value='<b>Waiting...</â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 features by importance:\n",
      "stime      0.135513\n",
      "ltime      0.121690\n",
      "dport      0.113955\n",
      "proto      0.059809\n",
      "seq        0.058971\n",
      "sport      0.057938\n",
      "flgs       0.052851\n",
      "sbytes     0.051870\n",
      "spkts      0.050684\n",
      "dpkts      0.050230\n",
      "pkts       0.045153\n",
      "state      0.040938\n",
      "mean       0.032215\n",
      "pkSeqID    0.022010\n",
      "dur        0.020890\n",
      "bytes      0.015328\n",
      "dbytes     0.014841\n",
      "min        0.012477\n",
      "sum        0.012227\n",
      "drate      0.009346\n",
      "dtype: float64\n",
      "\n",
      "Kept feature count: 19\n",
      "Dropped feature count: 5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "DATA_DIR = Path(\"balanced_output\")  # fixed path name\n",
    "DATA_FILE = \"chunk_int_data.csv\"\n",
    "TARGET_COLUMN = \"source\"  # change to the actual target column name\n",
    "IGNORE_COLUMNS = {\"saddr\", \"daddr\"}\n",
    "IMPORTANCE_THRESHOLD = 0.01\n",
    "RANDOM_STATE = 42\n",
    "N_ESTIMATORS = 300\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "\n",
    "stage_bar = widgets.IntProgress(value=0, min=0, max=6, description=\"Stage\", bar_style=\"info\")\n",
    "detail_label = widgets.HTML(value=\"<b>Waiting...</b>\")\n",
    "display(widgets.VBox([stage_bar, detail_label]))\n",
    "\n",
    "def _update(stage: int, text: str):\n",
    "    stage_bar.value = stage\n",
    "    detail_label.value = f\"<b>{text}</b>\"\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    _update(1, f\"Loading dataset: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    if TARGET_COLUMN not in df.columns:\n",
    "        raise ValueError(f\"Target column '{TARGET_COLUMN}' not found in {path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def _encode_non_numeric(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    converted = X.copy()\n",
    "    non_numeric_cols = converted.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "    for col in non_numeric_cols:\n",
    "        as_numeric = pd.to_numeric(converted[col], errors=\"coerce\")\n",
    "        if as_numeric.notna().any():\n",
    "            converted[col] = as_numeric.fillna(as_numeric.mean())\n",
    "        else:\n",
    "            converted[col] = converted[col].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "    return converted\n",
    "\n",
    "def prepare_features(df: pd.DataFrame):\n",
    "    _update(2, \"Preparing features\")\n",
    "    y = df[TARGET_COLUMN]\n",
    "    drop_cols = [col for col in IGNORE_COLUMNS if col in df.columns]\n",
    "    X = df.drop(columns=[TARGET_COLUMN] + drop_cols)\n",
    "\n",
    "    X = _encode_non_numeric(X)\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(y):\n",
    "        y = y.astype(\"category\").cat.codes\n",
    "\n",
    "    X = X.astype(\"float32\")\n",
    "    y = y.astype(\"int32\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def rank_features(X: pd.DataFrame, y: pd.Series) -> pd.Series:\n",
    "    _update(4, f\"Training RandomForest (n_estimators={N_ESTIMATORS})\")\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=N_ESTIMATORS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    clf.fit(X, y)\n",
    "    _update(5, \"Computing feature importances\")\n",
    "    importance_values = clf.feature_importances_\n",
    "    importance = pd.Series(importance_values, index=list(X.columns))\n",
    "    return importance.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset_path = DATA_DIR / DATA_FILE\n",
    "    df = load_dataset(dataset_path)\n",
    "    X, y = prepare_features(df)\n",
    "\n",
    "    _update(3, \"Splitting train/test\")\n",
    "    stratify_arg = y if len(np.unique(y)) > 1 else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=stratify_arg,\n",
    "    )\n",
    "\n",
    "    feature_importance = rank_features(X_train, y_train)\n",
    "    selected_features = feature_importance[\n",
    "        feature_importance >= IMPORTANCE_THRESHOLD\n",
    "    ].index.tolist()\n",
    "    dropped_features = feature_importance.index.difference(selected_features).tolist()\n",
    "\n",
    "    _update(6, \"Saving outputs\")\n",
    "    print(\"Top 20 features by importance:\")\n",
    "    print(feature_importance.head(20))\n",
    "\n",
    "    print(\"\\nKept feature count:\", len(selected_features))\n",
    "    print(\"Dropped feature count:\", len(dropped_features))\n",
    "\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    feature_importance.to_csv(DATA_DIR / \"feature_importance.csv\", header=[\"importance\"])\n",
    "    pd.Series(selected_features).to_csv(DATA_DIR / \"selected_features.csv\", index=False, header=[\"feature\"])\n",
    "\n",
    "    detail_label.value = \"<b>Done</b>\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d000bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data (train):\n",
      "    pkSeqID         stime  flgs  proto  sport  dport  pkts  state  \\\n",
      "0    66424  1.556087e+09     7    116  57056  10081     2     14   \n",
      "1   747890  1.556143e+09     7    116  57839   1900    24     14   \n",
      "2   139150  1.554314e+09     1    110  42100   7878     0      6   \n",
      "3   137880  1.556085e+09    11    110  61775  41952    16     18   \n",
      "4    81498  1.556085e+09     7    110  19824   6517     1     14   \n",
      "\n",
      "          ltime  seq       dur  spkts  source  \n",
      "0  1.556087e+09    0  4.964505      2       0  \n",
      "1  1.556143e+09    0  3.312139     24       1  \n",
      "2  1.554314e+09    0  0.000000      0       1  \n",
      "3  1.556085e+09    0  0.012719      7       1  \n",
      "4  1.556085e+09    0  0.000000      1       0  \n",
      "Wrote 1854724 rows to balanced_output\\extracted_features_chunk.csv\n",
      "Sample data (test):\n",
      "    pkSeqID         stime flgs proto  sport  dport  pkts state         ltime  \\\n",
      "0   805352  1.556135e+09   S0   tcp    802    802   120    S0  1.556135e+09   \n",
      "1   175623  1.554345e+09  OTH   tcp  42100   7878     0   OTH  1.554345e+09   \n",
      "2   608036  1.556425e+09  OTH   tcp   7678  50958     1   OTH  1.556425e+09   \n",
      "3   792525  1.556295e+09   SF   tcp  60126  49152    10    SF  1.556295e+09   \n",
      "4   310691  1.556186e+09   SF   tcp  59940     80    10    SF  1.556186e+09   \n",
      "\n",
      "   seq       dur  spkts  source  \n",
      "0    0  0.195980    120  normal  \n",
      "1    0  0.000000      0  normal  \n",
      "2    0  0.000000      1  normal  \n",
      "3    0  0.006923      5  normal  \n",
      "4    0  0.262083      5  attack  \n",
      "Wrote 1000 rows to balanced_output\\extracted_features_test.csv\n",
      "Wrote 1854724 rows to balanced_output\\extracted_features_chunk.csv\n",
      "Sample data (test):\n",
      "    pkSeqID         stime flgs proto  sport  dport  pkts state         ltime  \\\n",
      "0   805352  1.556135e+09   S0   tcp    802    802   120    S0  1.556135e+09   \n",
      "1   175623  1.554345e+09  OTH   tcp  42100   7878     0   OTH  1.554345e+09   \n",
      "2   608036  1.556425e+09  OTH   tcp   7678  50958     1   OTH  1.556425e+09   \n",
      "3   792525  1.556295e+09   SF   tcp  60126  49152    10    SF  1.556295e+09   \n",
      "4   310691  1.556186e+09   SF   tcp  59940     80    10    SF  1.556186e+09   \n",
      "\n",
      "   seq       dur  spkts  source  \n",
      "0    0  0.195980    120  normal  \n",
      "1    0  0.000000      0  normal  \n",
      "2    0  0.000000      1  normal  \n",
      "3    0  0.006923      5  normal  \n",
      "4    0  0.262083      5  attack  \n",
      "Wrote 1000 rows to balanced_output\\extracted_features_test.csv\n"
     ]
    }
   ],
   "source": [
    "dataset_path = DATA_DIR / DATA_FILE\n",
    "test_dataset_path = DATA_DIR / \"test_data.csv\"\n",
    "\n",
    "!\n",
    "columns_to_drop = [\"daddr\", \"saddr\", \"rate\", \"dbytes\", \"min\", \"sum\", \"bytes\", \"mean\", \"stddev\", \"max\", \"dpkts\", \"sbytes\", \"srate\", \"drate\"]\n",
    "\n",
    "def _process_file(input_path: Path, output_filename: str, label: str):\n",
    "    _update(1, f\"Loading dataset: {input_path.name}\")\n",
    "    frame = pd.read_csv(input_path)\n",
    "\n",
    "    _update(2, f\"Dropping specified columns from {label}\")\n",
    "    cleaned = frame.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "    print(f\"Sample data ({label}):\\n\", cleaned.head())\n",
    "\n",
    "    output_path = DATA_DIR / output_filename\n",
    "    _update(6, f\"Saving extracted features to {output_filename}\")\n",
    "    cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Wrote {len(cleaned)} rows to {output_path}\")\n",
    "    return cleaned, output_path\n",
    "\n",
    "df, output_path = _process_file(dataset_path, \"extracted_features_chunk.csv\", \"train\")\n",
    "test_df, test_output_path = _process_file(test_dataset_path, \"extracted_features_test.csv\", \"test\")\n",
    "\n",
    "detail_label.value = \"<b>Saved extracted_features_chunk.csv and extracted_features_test.csv</b>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e4334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
