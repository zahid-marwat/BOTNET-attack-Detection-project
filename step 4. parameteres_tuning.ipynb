{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519a835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Locating dataset under 'balanced_output'\n",
      "    -> Using dataset: balanced_output\\extracted_features_chunk.csv\n",
      "[+] Reading dataset into memory\n",
      "[+] Preprocessing data (encoding + cleaning)\n",
      "[+] Preprocessing data (encoding + cleaning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 00:04:37,720 - pyswarms.single.global_best - INFO - Optimize for 5 iters with {'c1': 1.7, 'c2': 1.7, 'w': 0.6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> Feature matrix shape: (1854724, 12)\n",
      "    -> Number of classes: 2\n",
      "    -> Using LightGBM base params: {'device': 'gpu', 'gpu_platform_id': 0, 'gpu_device_id': 0, 'tree_learner': 'serial', 'objective': 'binary'}\n",
      "[+] Initialising PSO optimizer\n",
      "[+] Running PSO for 5 iterations ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:  20%|██        |1/5, best_cost=-0.999"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> Objective evaluations: 24, current best cost: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:  40%|████      |2/5, best_cost=-0.999"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> Objective evaluations: 48, current best cost: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:  60%|██████    |3/5, best_cost=-0.999"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> Objective evaluations: 72, current best cost: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:  80%|████████  |4/5, best_cost=-0.999"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -> Objective evaluations: 96, current best cost: inf\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ImportError as exc:\n",
    "    raise SystemExit(\"lightgbm is required. Install it with `pip install lightgbm`.\") from exc\n",
    "\n",
    "try:\n",
    "    import pyswarms as ps\n",
    "except ImportError as exc:\n",
    "    raise SystemExit(\"pyswarms is required. Install it with `pip install pyswarms`.\") from exc\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# LightGBM hyperparameter bounds for PSO (Particle Swarm Optimisation)\n",
    "\n",
    "LOWER_BOUNDS = np.array([8, 3, 0.01, 200, 10, 0.5, 0.5, 0.0, 0.0], dtype=float)\n",
    "UPPER_BOUNDS = np.array([256, 16, 0.3, 1500, 120, 1.0, 1.0, 5.0, 5.0], dtype=float)\n",
    "DIMENSIONS = LOWER_BOUNDS.size\n",
    "\n",
    "# PSO configuration\n",
    "N_PARTICLES = 24\n",
    "PSO_ITERS = 5\n",
    "PSO_OPTIONS = {\"c1\": 1.7, \"c2\": 1.7, \"w\": 0.6}\n",
    "\n",
    "# Model persistence\n",
    "MODEL_OUTPUT_PATH = Path(\"balanced_output\") / \"lightgbm_pso_model.txt\"\n",
    "\n",
    "\n",
    "def log_stage(stage: str) -> None:\n",
    "    print(f\"[+] {stage}\")\n",
    "\n",
    "\n",
    "def locate_dataset() -> Path:\n",
    "    log_stage(\"Locating dataset under 'balanced_output'\")\n",
    "    base_dir = Path(\"balanced_output\")\n",
    "    if not base_dir.exists():\n",
    "        raise FileNotFoundError(\"The directory 'balanced_output' was not found.\")\n",
    "    candidates = [\n",
    "        base_dir / \"extracted_features_chunk\",\n",
    "        base_dir / \"extracted_features_chunk.csv\",\n",
    "        base_dir / \"extracted_features_chunk.parquet\",\n",
    "        base_dir / \"extracted_features_chunk.feather\",\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if candidate.exists():\n",
    "            if candidate.is_dir():\n",
    "                print(f\"    -> Skipping directory: {candidate}\")\n",
    "                continue\n",
    "            print(f\"    -> Using dataset: {candidate}\")\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"The dataset 'selected_features' was not found in 'balanced_output'.\")\n",
    "\n",
    "\n",
    "def read_dataset(path: Path) -> pd.DataFrame:\n",
    "    log_stage(\"Reading dataset into memory\")\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix in (\"\", \".csv\", \".txt\"):\n",
    "        return pd.read_csv(path)\n",
    "    if suffix == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    if suffix == \".feather\":\n",
    "        return pd.read_feather(path)\n",
    "    raise ValueError(f\"Unsupported file extension: {suffix or '(none)'}\")\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    log_stage(\"Preprocessing data (encoding + cleaning)\")\n",
    "    if \"source\" not in df.columns:\n",
    "        raise KeyError(\"The dataset must contain a 'source' column as the label.\")\n",
    "    y_raw = df[\"source\"]\n",
    "    X = df.drop(columns=[\"source\"]).copy()\n",
    "\n",
    "    cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    for col in cat_cols:\n",
    "        X[col] = pd.factorize(X[col])[0]\n",
    "\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(y_raw)\n",
    "\n",
    "    print(f\"    -> Feature matrix shape: {X.shape}\")\n",
    "    print(f\"    -> Number of classes: {np.unique(y).size}\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def particle_to_params(particle: np.ndarray) -> Dict[str, float]:\n",
    "    params = {\n",
    "        \"num_leaves\": int(np.round(np.clip(particle[0], LOWER_BOUNDS[0], UPPER_BOUNDS[0]))),\n",
    "        \"max_depth\": int(np.round(np.clip(particle[1], LOWER_BOUNDS[1], UPPER_BOUNDS[1]))),\n",
    "        \"learning_rate\": float(np.clip(particle[2], LOWER_BOUNDS[2], UPPER_BOUNDS[2])),\n",
    "        \"n_estimators\": int(np.round(np.clip(particle[3], LOWER_BOUNDS[3], UPPER_BOUNDS[3]))),\n",
    "        \"min_child_samples\": int(np.round(np.clip(particle[4], LOWER_BOUNDS[4], UPPER_BOUNDS[4]))),\n",
    "        \"subsample\": float(np.clip(particle[5], LOWER_BOUNDS[5], UPPER_BOUNDS[5])),\n",
    "        \"colsample_bytree\": float(np.clip(particle[6], LOWER_BOUNDS[6], UPPER_BOUNDS[6])),\n",
    "        \"reg_alpha\": float(np.clip(particle[7], LOWER_BOUNDS[7], UPPER_BOUNDS[7])),\n",
    "        \"reg_lambda\": float(np.clip(particle[8], LOWER_BOUNDS[8], UPPER_BOUNDS[8])),\n",
    "    }\n",
    "    if params[\"max_depth\"] > 0:\n",
    "        params[\"num_leaves\"] = max(2, min(params[\"num_leaves\"], 2 ** params[\"max_depth\"]))\n",
    "    return params\n",
    "\n",
    "\n",
    "def gpu_base_params(num_classes: int) -> Dict[str, float]:\n",
    "    common: Dict[str, float] = {\n",
    "        \"device\": \"gpu\",\n",
    "        \"gpu_platform_id\": 0,\n",
    "        \"gpu_device_id\": 0,\n",
    "        \"tree_learner\": \"serial\",\n",
    "    }\n",
    "    if num_classes <= 2:\n",
    "        common.update({\"objective\": \"binary\"})\n",
    "    else:\n",
    "        common.update({\"objective\": \"multiclass\", \"num_class\": num_classes})\n",
    "    return common\n",
    "\n",
    "\n",
    "def make_objective(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    base_params: Dict[str, float],\n",
    "    cv: StratifiedKFold,\n",
    ") -> Callable[[np.ndarray], np.ndarray]:\n",
    "    cache: Dict[Tuple[float, ...], float] = {}\n",
    "    eval_counter = {\"calls\": 0, \"best\": float(\"inf\")}\n",
    "\n",
    "    def objective(particles: np.ndarray, **_: object) -> np.ndarray:\n",
    "        costs = np.empty(particles.shape[0], dtype=float)\n",
    "        for idx, particle in enumerate(particles):\n",
    "            key = tuple(np.round(particle, 5))\n",
    "            if key in cache:\n",
    "                cost = cache[key]\n",
    "            else:\n",
    "                params = particle_to_params(particle)\n",
    "                model = lgb.LGBMClassifier(\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42,\n",
    "                    **base_params,\n",
    "                    **params,\n",
    "                )\n",
    "                try:\n",
    "                    score = cross_val_score(\n",
    "                        model,\n",
    "                        X,\n",
    "                        y,\n",
    "                        cv=cv,\n",
    "                        scoring=\"f1_macro\",\n",
    "                        n_jobs=-1,\n",
    "                    ).mean()\n",
    "                    cost = -score\n",
    "                except Exception:\n",
    "                    cost = 1.0\n",
    "                cache[key] = cost\n",
    "            costs[idx] = cost\n",
    "\n",
    "        eval_counter[\"calls\"] += particles.shape[0]\n",
    "        batch_best = costs.min()\n",
    "        if batch_best < eval_counter[\"best\"]:\n",
    "            eval_counter[\"best\"] = batch_best\n",
    "        print(\n",
    "            f\"    -> Objective evaluations: {eval_counter['calls']}, current best cost: {eval_counter['best']:.5f}\"\n",
    "        )\n",
    "        return costs\n",
    "\n",
    "    return objective\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    data_path = locate_dataset()\n",
    "    df = read_dataset(data_path)\n",
    "    X, y = preprocess(df)\n",
    "\n",
    "    num_classes = int(np.unique(y).size)\n",
    "    base_params = gpu_base_params(num_classes)\n",
    "    print(f\"    -> Using LightGBM base params: {base_params}\")\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    log_stage(\"Initialising PSO optimizer\")\n",
    "    objective = make_objective(X, y, base_params, cv)\n",
    "\n",
    "    optimizer = ps.single.GlobalBestPSO(\n",
    "        n_particles=N_PARTICLES,\n",
    "        dimensions=DIMENSIONS,\n",
    "        bounds=(LOWER_BOUNDS, UPPER_BOUNDS),\n",
    "        options=PSO_OPTIONS,\n",
    "        ftol=1e-4,\n",
    "        ftol_iter=10,\n",
    "    )\n",
    "\n",
    "    log_stage(f\"Running PSO for {PSO_ITERS} iterations ...\")\n",
    "    best_cost, best_position = optimizer.optimize(\n",
    "        objective,\n",
    "        iters=PSO_ITERS,\n",
    "        verbose=True,\n",
    "        print_step=1,\n",
    "    )\n",
    "    best_params = particle_to_params(best_position)\n",
    "\n",
    "    log_stage(\"Fitting final LightGBM model on full data (GPU)\")\n",
    "    best_model = lgb.LGBMClassifier(\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        **base_params,\n",
    "        **best_params,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        best_model.fit(X, y)\n",
    "    except lgb.basic.LightGBMError as exc:\n",
    "        print(\"[!] GPU training failed, retrying on CPU: \", exc)\n",
    "        base_params_cpu = dict(base_params)\n",
    "        base_params_cpu[\"device\"] = \"cpu\"\n",
    "        best_model = lgb.LGBMClassifier(\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            **base_params_cpu,\n",
    "            **best_params,\n",
    "        )\n",
    "        best_model.fit(X, y)\n",
    "\n",
    "    MODEL_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    booster = best_model.booster_\n",
    "    booster.save_model(str(MODEL_OUTPUT_PATH))\n",
    "    print(f\"    -> Saved LightGBM model to {MODEL_OUTPUT_PATH}\")\n",
    "\n",
    "    print(f\"\\nBest macro F1 (5-fold CV): {-best_cost:.4f}\")\n",
    "    print(\"Optimized hyperparameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8269d",
   "metadata": {},
   "source": [
    "Dataset extracted_features_chunk.csv (40k rows, 12 features, 2 classes) loaded and preprocessed; LightGBM configured for GPU (RTX 3090).\n",
    "\n",
    "Particle Swarm Optimization executed 5 iterations (120 objective evals); best macro-F1 ≈ 0.9976 found with position [44, 15, 0.0642, 1233, 21, 0.6443, 0.5442, 0.2085, 1.9372].\n",
    "\n",
    "Final LightGBM retrained on full data using those hyperparameters; warnings show the trees quickly hit stopping criteria (no further positive-gain splits), which simply reflects model convergence.\n",
    "\n",
    "Booster saved to lightgbm_pso_model.txt along with the optimized parameters listed in the notebook output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267473c",
   "metadata": {},
   "source": [
    "PSO outputs:\n",
    "\n",
    "num_leaves: 44\n",
    "max_depth: 15\n",
    "learning_rate: 0.0641720650\n",
    "n_estimators: 1233\n",
    "min_child_samples: 21\n",
    "subsample: 0.6442550582\n",
    "colsample_bytree: 0.5441757588\n",
    "reg_alpha: 0.2084502493\n",
    "reg_lambda: 1.9372339808\n",
    "\n",
    "These yielded a 5-fold macro-F1 of about 0.9976; the trained booster is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5038da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "data_path = locate_dataset()\n",
    "df = read_dataset(data_path)\n",
    "X, y = preprocess(df)\n",
    "\n",
    "encoder = LabelEncoder().fit(df[\"source\"])\n",
    "class_names = list(encoder.classes_)\n",
    "assert np.array_equal(y, encoder.transform(df[\"source\"])), \"Label encoding mismatch detected.\"\n",
    "\n",
    "num_classes = len(class_names)\n",
    "base_params = gpu_base_params(num_classes)\n",
    "base_params[\"device\"] = \"cpu\"\n",
    "\n",
    "best_params = {\n",
    "    \"num_leaves\": 44,\n",
    "    \"max_depth\": 15,\n",
    "    \"learning_rate\": 0.064172065,\n",
    "    \"n_estimators\": 1233,\n",
    "    \"min_child_samples\": 21,\n",
    "    \"subsample\": 0.6442550582,\n",
    "    \"colsample_bytree\": 0.5441757588,\n",
    "    \"reg_alpha\": 0.2084502493,\n",
    "    \"reg_lambda\": 1.9372339808,\n",
    "}\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    **base_params,\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"balanced_accuracy\": \"balanced_accuracy\",\n",
    "    \"precision_macro\": \"precision_macro\",\n",
    "    \"recall_macro\": \"recall_macro\",\n",
    "    \"f1_macro\": \"f1_macro\",\n",
    "}\n",
    "if num_classes == 2:\n",
    "    scoring[\"roc_auc\"] = \"roc_auc\"\n",
    "else:\n",
    "    scoring[\"roc_auc_ovr_weighted\"] = \"roc_auc_ovr_weighted\"\n",
    "\n",
    "cv_scores = cross_validate(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "metric_cols = [col for col in cv_scores if col.startswith(\"test_\")]\n",
    "fold_metrics = pd.DataFrame(cv_scores)[metric_cols].rename(columns=lambda c: c.replace(\"test_\", \"\"))\n",
    "summary_metrics = fold_metrics.agg([\"mean\", \"std\"])\n",
    "\n",
    "print(f\"Dataset shape: {X.shape} | Classes: {class_names}\")\n",
    "display(fold_metrics)\n",
    "display(summary_metrics)\n",
    "\n",
    "y_pred = cross_val_predict(model, X, y, cv=cv, n_jobs=-1)\n",
    "report_df = pd.DataFrame(\n",
    "    classification_report(y, y_pred, target_names=class_names, output_dict=True)\n",
    ").T\n",
    "display(report_df)\n",
    "\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    ")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix (5-fold out-of-fold)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if num_classes == 2:\n",
    "    y_proba = cross_val_predict(model, X, y, cv=cv, method=\"predict_proba\", n_jobs=-1)[:, 1]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\", linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", linewidth=1)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (5-fold out-of-fold)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y, y_proba)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(recall, precision, linewidth=2)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve (5-fold out-of-fold)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "model.fit(X, y)\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "importance_df = feature_importances.to_frame(name=\"importance\")\n",
    "display(importance_df)\n",
    "\n",
    "top_k = min(20, len(feature_importances))\n",
    "plt.figure(figsize=(8, max(4, top_k * 0.35)))\n",
    "sns.barplot(\n",
    "    x=feature_importances.iloc[:top_k],\n",
    "    y=feature_importances.index[:top_k],\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "plt.xlabel(\"Gain importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(f\"Top {top_k} Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff459a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
